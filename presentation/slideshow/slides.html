<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }

      img { width: 75%; }

      .right-col-1-third img { width: 100%; }

      .footnote {
        font-size: smaller;
        background: #eee;
        padding: 0.1% 3%;
      }

      .left-col-2-third {
        float: left;
        width: 65%;
      }

      .right-col-1-third {
        float: right;
        width: 32%;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: left, middle

# Machine Learning with TensorFlow

Mike Davis (mike.davis@asynchrony.com)

Example Code at [https://github.com/lightcycle/MachineLearningWithTensorFlow](https://github.com/lightcycle/MachineLearningWithTensorFlow)


---

# What is Machine Learning?

+ Methods for finding structure in or making predictions from data without explicitly programmed logic

+ Examples:
 + Image classification
 + Speech to text
 + Natural language processing
 + Spam detection
 + Recommendation engines

--

## Unsupervised vs. Supervised

+ Unsupervised methods find structure in unlabeled data
+ Supervised methods find a predictive function that fits provided input and output data
+ We'll focus on supervised learning

---

# Components of a Supervised Learning Solution

Supervised learning setups commonly include these components:

+ A **model function** mapping inputs to outputs, including configurable **parameters** that alter how the model works.

+ A **loss function** that quantifies how accurate the model is for given parameters, inputs, and outputs

+ An **optimization algorithm** that repeatedly tweaks the parameters and evaluates the loss function to improve the accuracy of the model

---

layout: true

# Example: Linear Regression

---

Imagine we have these measurements of cricket chirp frequency at certain temperatures:

.center[
![Dataset: Temp vs. Cricket Chirps](images/linear_model_data.png "Dataset: Temp vs. Cricket Chirps")]

---

## Model
Since we think there's a linear relationship, our model is the equation for a line:

.center[_predicted_outputs = weight * inputs + bias_]

## Parameters
* _weight_ controls the slope of the line
* _bias_ shifts the line up and down

---

## Loss Function
Mean squared error (MSE) is a common choice for regression loss functions. For each input temperature, we'll square the difference between the model's predicted chirp frequency and the measured chirp frequency. Then we'll take the average of those squared differences.

.center[_loss = average of (predicted_output - output)<sup>2</sup>_]

Squaring the differences has the effect of:
* Ensuring that the loss is positive
* Penalizing outliers

---

## Optimization Algorithm

Our optimization algorithm needs to determine the _weight_ and _bias_ values that result in a line best fitting our data.

**Gradient descent** is a common strategy:
1. Start from a reasonable, random set of parameter values
2. Determine the direction most likely to result in improvement, using partial derivatives of the loss function over each parameter 
3. Update the parameters slightly in the determined direction and repeat at step 2.

.footnote[
Note: Linear regression is a silly example for machine learning, since we could simply solve for the minimum of the loss function. But this isn't practical for more complex models with thousands of parameters.
]

---

.center[
![Plot: Linear Regression Loss](images/linear_model_parameters_surface.png "Plot: Linear Regression Loss")]

Starting from a random spot on this surface, gradient descent will eventually discover the optimal parameters.

---

layout: false

# TensorFlow Introduction

TensorFlow is Google's open-sourced library for these kinds of computations, first released Nov 2015.

Similar popular libraries are Theano, Torch, and Caffe. More recently released libraries include CNTK (Microsoft) and DSSTNE (Amazon).

--

With TensorFlow:
+ Data is stored in _n_-dimensional arrays called **tensors**
 + 0-dim for scalars
 + 1-dim for vectors
 + 2-dim for matrices
 + etc
+ Computations are defined as a **data flow graph** passing tensors between mathematical operations
 + Graph defined in a high-level language (only Python currently)
 + Graph computed by native code for performance

---

.left-col-2-third[

# Example: Linear Regression
Defining the data flow graph:
```python
# Placeholders for providing data to graph
X = tf.placeholder(tf.float32,
                   name = "temperature")
Y = tf.placeholder(tf.float32,
                   name = "chirp_freq")

# Model (Linear)
weight = tf.Variable(0., name = "weight")
bias = tf.Variable(0., name = "bias")
modeled_Y = tf.add(tf.mul(X, weight), bias)

# Loss Function (Mean Squared Error)
loss = tf.reduce_mean(
        tf.squared_difference(Y, modeled_Y))
```
]

.right-col-1-third[
![Graph: Linear Regression](images/linear_model_graph.png "Graph: Linear Regression")
]

---

layout: true

# Example: Linear Regression

---

Input to be fed to the graph for training:
```python
temp_f = [88.6, 71.6, 93.3, 84.3, 80.6, 75.2, 69.7, 71.6, 69.4, 
          83.3, 79.6, 82.6, 80.6, 83.5, 76.3]
cricket_chirps_per_s = [20, 16, 19.8, 18.4, 17.1, 15.5, 14.7, 
                        15.7, 15.4, 16.3, 15, 17.2, 16, 17, 14.4]
```
Creating the optimizer operation:
```python
learning_rate = 0.0001
training_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
```
Training the model for 50 steps:
```python
with tf.Session() as session:
    tf.initialize_all_variables().run()
    for step in range(50):
        session.run([training_op],
                    feed_dict={X: temp_f, Y: cricket_chirps_per_s})
```

---

Trained linear model results:

.center[
![Plot: Linear Regression Result](images/linear_model_result.png "Plot: Linear Regression Result")
]

---

TensorFlow takes care of several details for us, by examining the data flow graph:
+ Identifying the trainable ```weight``` and ```bias``` parameters we created with ```tf.Variable()```
+ Performing automatic differentiation to support gradient descent optimization
+ Distributing execution of the graph over available CPUs and GPUs

--

Additional features that take advantage of the graph include:
+ Saving and restoring all parameter values
+ Operations for logging values, images, and audio to Tensorboard
+ Distributing execution over multiple machines
+ Exporting trained model for use in production

---

layout: true

# Example: Neural Net

---

Neural net models are composed of nodes inspired by biological neurons:

.center[
![Diagram: Artificial Neuron](images/artificial_neuron.png "Diagram: Artificial Neuron")
]

Each input node's value is multiplied by a separate weight parameter, and the result is summed together with a bias parameter.

Finally, the result is transformed by an **activation function**.

.footnote[
Note: The activation function introduces _non-linearity_. Without it, the math performed by the node (or even a network of nodes) could be algebraically reduced to a simple linear equation. Which would produce a convoluted linear regression model.
]

---

The activation function can be anything, but two common choices are the _sigmoid_ and _rectified linear units (ReLU)_ functions.

.center[
![Plot: Activation Functions](images/activation_functions.png "Plot: Activation Functions")
]

---

Neural networks are constructed with one or more hidden layers of nodes between the input nodes and output nodes.

.center[
![Diagram: Neural Network](images/neural_network.png "Diagram: Neural Network")
]

Each hidden layer essentially combines instances of the activation function shape, scaled and shifted by node weights and bias.

.footnote[
The _universal approximation theorem_ shows that with an appropriate activation function and enough nodes, a single layer can approximate any function.
]

    </textarea>
    <script src="js/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>