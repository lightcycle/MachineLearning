<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }

      img { width: 75%; }

      .footnote {
        font-size: smaller;
        background: #eee;
        padding: 0.1% 3%;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: left, middle

# Machine Learning with TensorFlow

Mike Davis (mike.davis@asynchrony.com)

Example Code at [https://github.com/lightcycle/MachineLearningWithTensorFlow](https://github.com/lightcycle/MachineLearningWithTensorFlow)


---

# What is Machine Learning?

+ Methods for finding structure in or making predictions from data without explicitly programmed logic

+ Examples:
 + Image classification
 + Speech to text
 + Natural language processing
 + Spam detection
 + Recommendation engines

--

## Unsupervised vs. Supervised

+ Unsupervised methods find structure in unlabeled data
+ Supervised methods find a predictive function that fits provided input and output data
+ We'll focus on supervised learning

---

# Components of a Supervised Learning Solution

Supervised learning setups commonly include these components:

+ A **model function** mapping inputs to outputs, including configurable **parameters** that alter how the model works.

+ A **loss function** that quantifies how accurate the model is for given parameters, inputs, and outputs

+ An **optimization algorithm** that repeatedly tweaks the parameters and evaluates the loss function to improve the accuracy of the model

---

layout: true

# Example: Linear Regression

---

Imagine we have these measurements of cricket chirp frequency at certain temperatures:

.center[
![Dataset: Temp vs. Cricket Chirps](images/linear_model_data.png "Dataset: Temp vs. Cricket Chirps")]

---

## Model
Since we think there's a linear relationship, our model is the equation for a line:

.center[_predicted_outputs = weight * inputs + bias_]

## Parameters
* _weight_ controls the slope of the line
* _bias_ shifts the line up and down

---

## Loss Function
Mean squared error (MSE) is a common choice for regression loss functions. For each input temperature, we'll square the difference between the model's predicted chirp frequency and the measured chirp frequency. Then we'll take the average of those squared differences.

.center[_loss = average of (predicted_output - output)<sup>2</sup>_]

Squaring the differences has the effect of:
* Ensuring that the loss is positive
* Penalizing outliers

---

## Optimization Algorithm

Our optimization algorithm needs to determine the _weight_ and _bias_ values that result in a line best fitting our data.

Most algorithms perform **gradient descent**:
1. Start from a reasonable, random set of parameter values
2. Use partial derivatives of the loss function over each parameter to determine the direction most likely to result in improvement
3. Update the parameters slightly in the determined direction and repeat at step 2.

.footnote[
Note: Linear regression is a silly example for machine learning, since we could simply solve for the minimum of the loss function. But this isn't possible for more complex models with thousands of parameters.
]

---

.center[
![Plot: Linear Regression Loss](images/linear_model_parameters_surface.png "Plot: Linear Regression Loss")]

Starting from a random spot on this surface, gradient descent will eventually discover the optimal parameters.

    </textarea>
    <script src="js/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>